---
title: Simulate Fog/Moon Effect in Genshin Effect Via ReShade
date: 2024-04-30 17:00:00 -0700
authors: [<author1_id>, <author2_id>, <author3_id>, <author4_id>]   # for multiple entries
math: true
image:
  path: /images/night1.png
---

To enhance the atmosphere while reading about our project, you may **play the following background music**:

<audio controls>
  <source src="/assets/audio/audio.mp3" type="audio/mp3">
  Your browser does not support the audio element.
</audio>

This project involves injecting custom shaders into the game Genshin Impact, implementing unique weather effects like foggy conditions during the day and night, as well as blood moons and lunar eclipses.

In the first part, we added custom moon and color filters to implement lunar eclipse and blood moon effects.

In the second part, we explored in-depth the simulation of foggy weather. We implemented adaptive depth fog, which can adjust according to the environment to enhance lighting effects at night.

***

Injecting custom shaders into Genshin Impact to add fog and moon effects required bypassing the game's built-in anti-cheat system, which prevents modifications to the rendering pipeline. The [Genshin-Impact-ReShade](https://github.com/sefinek24/Genshin-Impact-ReShade) repository on GitHub solved this challenge.

By following the instructions and utilizing the tools provided in the repository, we successfully installed the necessary components on a Windows machine. This allowed us to inject our custom shaders, written using the ReShade FX shading language, into Genshin Impact.

ReShade FX is a shading language and compiler that shares similarities with HLSL (High-Level Shading Language). With the ability to inject custom shaders, we were able to implement the desired fog and moon effects in Genshin Impact. These effects enhance the visual atmosphere and immersion of the game, creating a more captivating experience for players.

However, it's crucial to acknowledge that modifying the game's rendering pipeline through shader injection may violate Genshin Impact's terms of service or anti-cheat policies. Mengjunâ€™s account is unfortunately banned temporarily when we were working on the project.

## Part I Phantom Moon

### Introduction
Our goal is to add the effects of lunar eclipses and blood moons to the game. However, in Genshin Impact, almost all environmental elements, including clouds and the moon, are fixed textures, which limits the effectiveness of Reshade. Reshade primarily affects the overall post-processing effects of the image and cannot directly modify in-game models or specific object details. For example, you can use Reshade to enhance the overall sky effect, making it appear more vibrant or better harmonized with the background, but you may not be able to individually adjust the moon or clouds. Therefore, we decided to inject our own textures to add a fully customizable moon.

Unfortunately, due to Genshin Impact's unique art style in the industry, it is difficult to find suitable moon textures online that match the game's aesthetic. As a result, we have to focus more on implementing the relevant effects for the moon.

### Adding Our Own Moon

![Moon Texture](/images/moon.png){: width="400" height="300" .w-50 .right}

In the first step of implementing the shader, we focused on adding a custom moon to the game and providing adjustable parameters to control its appearance and behavior. To achieve this, we utilized ReShade's texture and sampler features to load a custom moon texture and apply it to the game's rendering pipeline.

We started by defining a texture and specifying its source image file, width, height, and format. This texture serves as the visual representation of the moon. We also created a corresponding sampler to access and sample the moon's texture.

### Customizing the Moon's Size and Position

![Moon Stretch](/images/stretch.png){: width="400" height="300" .w-50 .left}

To make the moon adjustable, we introduced several uniform variables that control various aspects of its appearance and position. One of the key variables is the moon position, which allows adjusting the position of the moon on the screen using X, Y, and Z coordinates. The X and Y coordinates determine the moon's horizontal and vertical position within the screen space, while the Z coordinate represents the depth or distance of the moon from the camera.

In the pixel shader responsible for rendering the moon, we calculate the sampling coordinates based on the moon's position and size. The shader takes the current pixel's texture coordinates (texcoord) and subtracts the moon's position (stored in the uniform variable) from them. This effectively shifts the texture coordinates relative to the moon's position.

To adjust the size of the moon, we divide the shifted texture coordinates by the moon size uniform variables (horizontal and vertical size). This scaling operation allows us to control the moon's dimensions independently.

Furthermore, to ensure proper aspect ratio and alignment, we multiply the horizontal texture coordinate by the ratio of the screen width to height (BUFFER_WIDTH * BUFFER_RCP_HEIGHT). This step accounts for any differences in the screen's aspect ratio. Finally, we add 0.5 to the scaled and adjusted texture coordinates to center the moon texture correctly.

By using these calculated sampling coordinates, we can sample the moon texture at the desired position and size. The sampled moon color is then blended with the original frame color based on the moon's opacity and depth range.

### Depth Control

![Moon Hide](/images/hide.png){: width="400" height="300" .w-50 .right}

To implement depth control and allow adjusting the moon's position relative to other objects in the game, such as mountains, we utilize the depth information provided by the game's rendering pipeline. The depth value of each pixel represents its distance from the camera. By comparing the pixel's depth value with a defined depth range for the moon, we can determine whether the moon should be visible at that pixel or not.

We introduce two adjustable parameters: the moon's depth position and the depth range. The moon's depth position represents its distance from the camera, while the depth range defines the range of depth values within which the moon is visible. In the shader code, we calculate the minimum and maximum depth values for the moon's visibility range based on the moon's depth position and the specified depth range. If a pixel's depth value falls within this range, we proceed with rendering the moon at that position. Otherwise, the moon is not rendered, and the original pixel color is used.

To create a smooth transition, we calculate the moon's opacity based on the difference between the pixel's depth and the moon's depth position. The opacity gradually decreases as the pixel's depth moves away from the moon's depth, ensuring a seamless blend with the surrounding environment.

By adjusting the moon's depth position and depth range parameters, we can control the moon's position relative to other objects in the game. Setting the moon's depth position to a smaller value will make it appear closer to the camera and in front of objects with greater depth values, while increasing the depth position will make the moon appear farther away and behind objects with smaller depth values.

### Color Control

![Moon Blood](/images/inverse.png){: width="400" height="400" .w-50 .left}

![Moon Inverse](/images/blood.png){: width="400" height="400" .w-50 .left}

To implement color inversion and turn the moon red, we use color manipulation techniques in the shader code.

For color inversion, we introduce a parameter that controls the intensity of the inversion effect. In the shader, we calculate the inverted color by subtracting the moon's original color from the maximum color value (usually 1.0). We then perform a linear interpolation between the original color and the inverted color using the inversion intensity parameter. This allows for a smooth transition between the original and inverted colors based on the intensity value.

To turn the moon red, we define a target red color using RGB values. In the shader, we perform a linear interpolation between the moon's original color and the target red color using a parameter that controls the intensity of the red coloring effect. This parameter determines the strength of the red tint applied to the moon.

The linear interpolation is achieved using a built-in function that takes the original color, target color, and a blending factor as arguments. By adjusting the blending factor (i.e., the red coloring intensity parameter), we can control the amount of red tint applied to the moon.

By exposing the color inversion intensity and red coloring intensity parameters to the user interface, we allow users to interactively adjust the moon's color effects. Users can modify these parameters to achieve the desired appearance of the moon, whether it's a partially inverted color or a fully red moon.

### Bloom Effect

To simulate the light around the moon after color inversion, we utilize a bloom mask in the shader. The bloom mask is generated based on the moon's alpha value and a specified threshold. By comparing the moon's alpha value with the threshold, we isolate the bright areas of the moon that will contribute to the bloom effect. The resulting bloom mask is then used to intensify the moon's color, creating a glowing effect around it. This bloom effect is applied after the color inversion step, allowing the inverted moon to emit a bright and luminous light. The bloom mask ensures that only the desired areas of the moon contribute to the glow, providing control over the intensity and spread of the light around the moon.

### Magic Sky

To simulate the effect brought by the changing of the moon, we need to precisely control the color of the sky, we utilize depth-based color manipulation in the shader. By comparing the depth value of each pixel with a specified depth range, we can isolate the sky region and apply color changes specifically to that area.

In the shader, we define a depth range that represents the distance from the camera at which the sky is located. We also specify a target sky color that will be used to tint the sky. For each pixel being processed, we retrieve its depth value using the game's depth buffer.

If the pixel's depth value falls within the specified sky depth range, we consider it as part of the sky region. We then perform a linear interpolation between the pixel's original color and the target sky color based on an intensity parameter. This parameter controls the strength of the sky color tint, allowing for a gradual blend between the original color and the desired sky color.

By applying the color change only to pixels within the sky depth range, we ensure that other parts of the game image, such as characters, objects, and landscapes, remain unaffected. The depth-based approach allows for precise control over the sky region, enabling us to modify its color independently.

To create a smooth transition between the sky and non-sky regions, we can adjust the depth range and intensity parameters. A larger depth range will include more pixels as part of the sky, while a smaller range will restrict the color change to a narrower portion of the image. The intensity parameter determines the strength of the sky color tint, allowing for subtle or dramatic color changes.

### Results

![eclipse-front](/images/eclipse-front.png){: width="3840" height="2160"}

![eclipse-back](/images/eclipse-back.png){: width="3840" height="2160"}

![eclipse](/images/eclipse.png){: width="3840" height="2160"}
_Lunar eclipse_

![blood1](/images/blood1.png){: width="3840" height="2160"}

![blood2](/images/blood2.png){: width="3840" height="2160"}

![blood3](/images/blood3.png){: width="3840" height="2160"}
_Blood moon_

### Problems We Encountered

When we first imported the moon texture into the shader, we noticed that the displayed moon had an unusual and distorted ratio. The moon appeared stretched or squished, indicating that the texture was not being mapped correctly onto the screen. This visual discrepancy broke the immersion and realism of the moon's appearance in the game.

To address the issue of the incorrect moon texture ratio, we realized that we needed to take into account the screen's aspect ratio when displaying the moon. The aspect ratio represents the proportional relationship between the width and height of the screen.

In the shader code, we introduced a calculation to adjust the moon's texture coordinates based on the screen's aspect ratio. Specifically, we multiplied the moon's horizontal texture coordinate by the ratio of the screen's width to its height. This adjustment ensures that the moon texture is mapped correctly onto the screen, regardless of the screen's dimensions.

### Lessons Learned

Through the development of this shader project, we gained valuable knowledge and experience in writing ReshadeFX code and creating shaders using a different rendering engine. This project provided us with a deeper understanding of various aspects of shader programming.

We learned how to implement advanced techniques such as depth-based effects, color manipulation, and bloom effects to enhance the visual quality and realism of the game's graphics. Moreover, we explored the process of creating custom textures and learned how to efficiently manage and sample textures within the shader code.

***

## Part II Spirit Fog

### Introduction

![Cover](/images/cover.png){: width="400" height="400" .w-50 .right}

We designed a fog effect algorithm based on depth information, integrating depth adjustment and light source detection, to simulate the visual effect of light diffusion. This implementation uses the Reshade framework and supports real-time parameter adjustment, enhancing the dynamic and interactive visual effects.

### Fog Blending

To simulate fog effects, we injected shaders into Genshin imapert. We found a publicly and legally usable Genshin-Impact-ReShade repository on GitHub, which can be applied to the rendering pipeline and implements rendering effects using the ReShade FX compiler. We registered a new Genshin account and tested the operation of Genshin with Reshade.

Next, we implemented the basic fog simulation. Our concept is that since the actual effect of fog is increasingly hazy from near to far, it is feasible to process the depth information of pixels. The depth fog is a viable approach.

```glsl
depth = ReShade::GetLinearizedDepth(texcoord).r;
```

First, we obtain the linearized depth information from the current texture coordinates.

```hlsl
fogFactor = clamp(saturate(depth - FogStart) * FogCurve, 0.0, MaxFogFactor);
```

Then, calculate the fog factor (fogFactor) based on depth information, which determines the concentration of the fog. The fog factor is calculated by subtracting the depth from the fog's start position (FogStart), multiplying by the fog curve (FogCurve), and then clamping the value between 0 and the maximum fog factor using the clamp function.

```hlsl
fogColor = lerp(sceneColor, float4(FogColor, 1.0), fogFactor);
```

After obtaining the fog factor, we process the blending of fog effects, making objects in the distance appear as if covered by fog. The original color of the scene and the fog color (FogColor) are linearly interpolated based on the fog factor to generate the final color output.

This achieves a depth fog model, which performs well during the day.

### Results Of Daylight

![day1](/images/day1.png){: width="3840" height="2160"}
_Origin_

![day2](/images/day2.png){: width="3840" height="2160"}
_Default fog_

![day3](/images/day3.png){: width="3840" height="2160"}
_Adjusted Range_

![day4](/images/day4.png){: width="3840" height="2160"}
_Adjusted Factor_

![day5](/images/day5.png){: width="3840" height="2160"}
_Adjusted Curve_

Moreover, we set parameters that can be adjusted based on the actual environment to vary the range of the fog and the density effect of the fog factor, adapting it better.

### Problems We Encountered

![Idea](/images/idea.PNG){: width="320" height="300" .w-50 .right}

However, when switching to nighttime, the depth fog simulation did not perform well. Despite using the same factors as during the day, it did not resemble fog at all.

This is because in reality, during the day when there is sufficient light, observers perceive a "foggy day" through the contrast of distant and nearby objects brought by a sense of layering. At night, due to the lack of light, the layering of the scene is not prominent, and observers' attention is more focused on positions in the scene with light sources, where fog particulates should illuminate around the light sources. Since depth fog does not handle light sources, it does not look like a foggy day.

Next, we implemented nighttime fog, requiring the capture and blurring of light sources.

### Light Source Detection

As part of the rendering pipeline, shaders cannot directly access light sources in the scene. We keenly observed that at night, the brightest "object surfaces" are precisely where observers focus and judge the fog effect. Therefore, our code detects the "brightest parts" of the scene:

As part of the rendering pipeline, shaders cannot directly access light sources in the scene. We keenly observed that at night, the brightest "object surfaces" are precisely where observers focus and judge the fog effect. Therefore, our code detects the "brightest parts" of the scene:

$$
L(x, y) = 
\begin{cases} 
1 & \text{if } B(x, y) > T, \\
0 & \text{otherwise}
\end{cases}
$$

Where \( B(x, y) \) is obtained through the following brightness calculation:

$$
B(x,y) = \text{dot}(\text{tex2D}(LightDetectSampler, \text{texcoord}).rgb, \text{float3}(0.299, 0.587, 0.114))
$$

$T$ is the light source threshold LightSourceThreshold. When $B(x, y)$ exceeds $T$, we consider that pixel as a light source. Obtain the current pixel's color value (lowResColor) from a low-resolution texture (LightDetectSampler) and calculate its brightness using the weighted average formula of the Y channel in the YUV color image conversion.

The weights float3(0.299, 0.587, 0.114) correspond to the contributions of the R, G, and B color channels to human perception of brightness. The brightness is then compared with the set brightness threshold, and if it is a "brighter pixel," it is packed into a container for deeper processing.

Similarly, the threshold and range for light source detection can be set, allowing it to adapt to light sources under different scene brightness levels, and to exclude interference from nearby or distant sources.

### Light Fog Simulation

After obtaining bright pixels, as light areas in fog get darker with distance, brightness reduction based on pixel depth is applied, followed by halo effects on surrounding pixels.

Our approach is to apply a filter that create a blur around the light source. There are several blurring filters to be used. Our final implementation is the 2nd order Bessel Filter which can provide a good effect to diffuse the light around the light source while preserve the clarity around the dark area.

$$
H(s) = \frac{3}{s^2 + 3s + 3}
$$

Instead of using a 2d filter, to increase performance, the implementation used 2 1d filters in vertical and horizontal directions.

Several Parameters has been added to the Bessel Filter to customize the  glowing effect. Similar to the  depth based fog effect, there are also intensity to  control how strong the effect is.  The shader can also control the radius of the bloom, the situation, the density curve. The most important is that there is a threshold  to control which part of the scene is applied by the filter.

### Problems We Encountered

The first problem I encountered is how to choose a filter. There are several filters could be used. The first kind of filter that came to my mind is Gaussian Filter.  The initial implementation is to downsample and then upsample several layers. However, this implementation will blur up the whole scene and I cannot have specific control to the filter. There are also other filters. But I used Bessel filter because it has a gradual rolloff for blurring.

### Final Results

Finally, we achieved a better simulation of a foggy night scene.

![night1](/images/night1.png){: width="3840" height="2160"}
_Night Origin_

![night2](/images/night2.png){: width="3840" height="2160"}
_Night Fog_

![night3](/images/night3.png){: width="3840" height="2160"}
_Light Detection_

![night4](/images/night4.png){: width="3840" height="2160"}
_Light Glow_

### Lessons Learned

One of the key insights from this project was the importance of adapting visual effects algorithms to different environmental conditions. We enhanced our technical skills in shader programming and visual effects, deepening our understanding of the interplay between light, perception, and atmospheric conditions in real-time graphics. These experiences will guide our future projects.
